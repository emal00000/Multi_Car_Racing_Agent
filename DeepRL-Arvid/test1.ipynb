{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"terminated\")\n",
    ")\n",
    "Batch = namedtuple(\n",
    "    \"Batch\", (\"states\", \"actions\", \"rewards\", \"next_states\", \"terminateds\")\n",
    ")\n",
    "\n",
    "\n",
    "def state_encoder(state: torch.Tensor):\n",
    "    encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "    if len(state.shape) == 3:\n",
    "        state = state.unsqueeze(1)\n",
    "    elif len(state.shape) == 2:\n",
    "        state = state.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    x = encoder(state[:,:, :-1, :]).squeeze(0) # encoding track info\n",
    "    car_info = state[:, :, -1, :].squeeze((0,1))\n",
    "    return torch.cat((x, car_info), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity: int, batch_size: int):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = deque([], maxlen=capacity)\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data.append(Transition(*args))\n",
    "\n",
    "    def sample(self) -> Batch:\n",
    "        sample = random.choices(self.data, k=self.batch_size)\n",
    "        states, actions, rewards, next_states, terminateds = list(zip(*sample))\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        terminateds = torch.tensor(np.array(terminateds), dtype=torch.float32)\n",
    "        return Batch(states, actions, rewards, next_states, terminateds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    \"\"\"\n",
    "    SAC uses Q(s,a) instead of DQN's Q(s, :)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.state_encoder = state_encoder\n",
    "        self.dense_net = nn.Sequential(  \n",
    "                nn.Linear(518, 600),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(600, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        embedding = self.state_encoder(states)\n",
    "        x = torch.cat((embedding, actions.view(actions.shape[0], -1)), dim=1)\n",
    "        x = self.dense_net(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = state_encoder\n",
    "        self.fc = nn.Linear(515, 600)\n",
    "        self.mu = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 3)\n",
    "        )\n",
    "        self.std = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Reparameterized and squashed sampling \n",
    "        # Returns actions and log probabilities\n",
    "        encoding = self.encoder(state)\n",
    "        encoding = self.fc(encoding)\n",
    "        gaussian = torch.distributions.Normal(self.mu(encoding), torch.abs(self.std(encoding)))\n",
    "        u = gaussian.rsample()\n",
    "        a = torch.tanh(u)\n",
    "        \n",
    "        log_pi_u = gaussian.log_prob(u).sum(axis=-1)\n",
    "        inv_det_jacobian = torch.sum(\n",
    "            2 * (np.log(2) - u - nn.functional.softplus(-2 * u)), dim=-1\n",
    "        )\n",
    "        log_prob_a = log_pi_u - inv_det_jacobian\n",
    "        return a, log_prob_a\n",
    "\n",
    "\n",
    "class SacAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    SAC agent: \n",
    "        Policy, two Q, two target Qs\n",
    "        Three optimizers\n",
    "        Memory\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.lambda_ = 0.005\n",
    "        self.gamma = 0.99\n",
    "        self.alpha = 1.0\n",
    "\n",
    "        self.memory = ReplayMemory(capacity=100_000, batch_size=64)\n",
    "        self.pi = GaussianPolicy()\n",
    "        self.qs = [Qfunction(), Qfunction()]\n",
    "        self.tqs = [Qfunction(), Qfunction()]\n",
    "\n",
    "        self.optimizer_pi = torch.optim.Adam(self.pi.parameters(), lr=1e-3)\n",
    "        self.q_optimizers = [torch.optim.Adam(self.qs[0].parameters(), lr=1e-3),\n",
    "                       torch.optim.Adam(self.qs[1].parameters(), lr=1e-3)]\n",
    "        self.clone()\n",
    "\n",
    "\n",
    "    @torch.no_grad\n",
    "    def clone(self):\n",
    "        # Hard update (for initial copy)\n",
    "        self.tqs[0].load_state_dict(self.qs[0].state_dict())\n",
    "        self.tqs[1].load_state_dict(self.qs[1].state_dict())\n",
    "\n",
    "    @torch.no_grad\n",
    "    def soft_update(self):\n",
    "        # Soft update of targets\n",
    "        # Network 1\n",
    "        for target_param, Qnet_param in zip(self.tqs[0].parameters(), self.qs[0].parameters()):\n",
    "            target_param.data.copy_((1.0 - self.lambda_) * target_param.data + self.lambda_ * Qnet_param.data)\n",
    "        # Network 2\n",
    "        for target_param, Qnet_param in zip(self.tqs[1].parameters(), self.qs[1].parameters()):\n",
    "            target_param.data.copy_((1.0 - self.lambda_) * target_param.data + self.lambda_ * Qnet_param.data)\n",
    "\n",
    "    @torch.no_grad\n",
    "    def sample_action(self, state):\n",
    "        # Sample single action from the policy\n",
    "        action, _ = self.pi(state)\n",
    "        action[1:3] = 0.5 * action[1:3] + 0.5 # normalazing gas and braking to [0, 1] \n",
    "        return action\n",
    "\n",
    "    def update(self, batch: Batch):\n",
    "        # Update all parameters given batch\n",
    "        states, actions, rewards, next_states, terminateds = batch\n",
    "\n",
    "        # Update Qs\n",
    "        q_est_0 = self.qs[0](states, actions)\n",
    "        q_est_1 = self.qs[1](states, actions)\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.pi(next_states)\n",
    "            next_q_0 = self.tqs[0](next_states, next_actions)\n",
    "            next_q_1 = self.tqs[1](next_states, next_actions)\n",
    "            min_q = torch.min(next_q_0, next_q_1)\n",
    "            target = rewards.reshape(-1, 1) + self.gamma * (1 - terminateds.reshape(-1, 1)) * (\n",
    "                min_q - self.alpha * next_log_probs.reshape(-1, 1)\n",
    "            )\n",
    "        for q_est, opt in zip([q_est_0, q_est_1], self.q_optimizers):\n",
    "            q_loss = nn.functional.mse_loss(q_est, target)\n",
    "            q_loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # Update policy\n",
    "        # Freeze qs for efficiency\n",
    "        self.qs[0].requires_grad_(False)\n",
    "        self.qs[1].requires_grad_(False)\n",
    "\n",
    "        acts, log_probs = self.pi(states) # With states sampled from buffer\n",
    "        min_q = torch.min(self.qs[0](states, acts), self.qs[1](states, acts))\n",
    "        pi_loss = torch.mean(self.alpha * log_probs - min_q)\n",
    "        pi_loss.backward()\n",
    "\n",
    "        self.optimizer_pi.step()\n",
    "        self.optimizer_pi.zero_grad()\n",
    "\n",
    "        # Unfreeze qs again\n",
    "        self.qs[0].requires_grad_(True)\n",
    "        self.qs[1].requires_grad_(True)\n",
    "\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        # Store transition\n",
    "        self.memory.add(state, action, reward, next_state, terminated)\n",
    "\n",
    "    def learn(self):\n",
    "        # Just to call agent.learn() from some loop\n",
    "        batch = self.memory.sample()\n",
    "        self.update(batch)\n",
    "\n",
    "    def save(self, filename: str):\n",
    "        torch.save(self.pi.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename: str):\n",
    "        self.pi.load_state_dict(torch.load(filename, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC algorithm\n",
    "def SAC(env, num_episodes: int, num_experiments: int, min_alpha: float, tau: float, render_mode=None):\n",
    "    dfs = []\n",
    "    for exp in range(num_experiments):\n",
    "        scores = deque([], maxlen=10)\n",
    "        ma10 = np.zeros(num_episodes)\n",
    "        agent = SacAgent()\n",
    "        for e in range(num_episodes):\n",
    "            print(f\"SAC Running experiment {exp}, episode {e}\", end=\"\\r\")\n",
    "            state = env.reset()\n",
    "            terminated = False\n",
    "            score = 0\n",
    "            step = 0\n",
    "            while not (terminated):\n",
    "                action = agent.sample_action(state)\n",
    "                next_state, reward, terminated, _ = env.step(action.numpy())\n",
    "                next_state = next_state.unsqueeze(0)\n",
    "                agent.store(state, action, reward, next_state, terminated)\n",
    "                agent.learn()\n",
    "                agent.soft_update()\n",
    "                score += reward\n",
    "                step += 1\n",
    "                if step == 100:\n",
    "                    torch.save(next_state, \"next_state.pt\")\n",
    "                    print(\"saved next state\")\n",
    "                state = next_state\n",
    "                if render_mode is not None:\n",
    "                    env.render(mode=render_mode)\n",
    "            agent.alpha = max(min_alpha, agent.alpha * tau)\n",
    "            scores.append(score)\n",
    "            ma10[e] += np.mean(scores)\n",
    "\n",
    "        dfs.append(\n",
    "            pd.DataFrame(\n",
    "                {\"exp\": exp, \"episode\": np.arange(num_episodes), \"MA10\": ma10}\n",
    "            )\n",
    "        )\n",
    "        agent.save(f\"CarTest_Experiment_{exp}.txt\")\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/MultiCar/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1111..1393 -> 282-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libGL error: No matching fbConfigs or visuals found\n",
      "libGL error: failed to load driver: swrast\n"
     ]
    },
    {
     "ename": "ContextException",
     "evalue": "Could not create GL context",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiCarRacing-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_agents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCCW\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                     use_random_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, backwards_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, h_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m      6\u001b[0m                     use_ego_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     SAC_exp \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_experiments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mSAC\u001b[0;34m(env, num_episodes, num_experiments, min_alpha, tau, render_mode)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC Running experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/gym/wrappers/time_limit.py:25\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepRL/multi_car_racing/gym_multi_car_racing/multi_car_racing.py:440\u001b[0m, in \u001b[0;36mMultiCarRacing.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m wheel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcars[car_id]\u001b[38;5;241m.\u001b[39mwheels:\n\u001b[1;32m    438\u001b[0m         wheel\u001b[38;5;241m.\u001b[39mcar_id \u001b[38;5;241m=\u001b[39m car_id\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/DeepRL/multi_car_racing/gym_multi_car_racing/multi_car_racing.py:463\u001b[0m, in \u001b[0;36mMultiCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mFPS, \u001b[38;5;241m6\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mFPS\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents)\n\u001b[1;32m    466\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepRL/multi_car_racing/gym_multi_car_racing/multi_car_racing.py:630\u001b[0m, in \u001b[0;36mMultiCarRacing.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    628\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_car_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents):\n\u001b[0;32m--> 630\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_car_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(result, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/DeepRL/multi_car_racing/gym_multi_car_racing/multi_car_racing.py:644\u001b[0m, in \u001b[0;36mMultiCarRacing._render_window\u001b[0;34m(self, car_id, mode)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Performs the actual rendering for each car individually. \u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    mode(str): Rendering mode.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer[car_id] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rendering\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer[car_id] \u001b[38;5;241m=\u001b[39m rendering\u001b[38;5;241m.\u001b[39mViewer(WINDOW_W, WINDOW_H)\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer[car_id]\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mset_caption(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcar_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:25\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    Cannot import pyglet.\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    HINT: you can install pyglet directly via \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pyglet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    But if you really just want to install all Gym dependencies and not have to think about it,\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -e .[all]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install gym[all]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m will do it.\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m    Error occurred while running `from pyglet.gl import *`\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m    HINT: make sure you have OpenGL install. On Ubuntu, you can run \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapt-get install python-opengl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m    If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre running on a server, you may need a virtual frame buffer; something like this should work:\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxvfb-run -s \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m-screen 0 1400x900x24\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m python <your_script.py>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m)\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/gl/__init__.py:244\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyglet_doc_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyglet.window\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;129;01mand\u001b[39;00m _pyglet\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshadow_window\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# trickery is for circular import\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     _pyglet\u001b[38;5;241m.\u001b[39mgl \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/window/__init__.py:1880\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyglet_doc_run:\n\u001b[1;32m   1879\u001b[0m     pyglet\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m-> 1880\u001b[0m     \u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_shadow_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/gl/__init__.py:220\u001b[0m, in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Window\n\u001b[0;32m--> 220\u001b[0m _shadow_window \u001b[38;5;241m=\u001b[39m \u001b[43mWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m _shadow_window\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m app\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/window/xlib/__init__.py:165\u001b[0m, in \u001b[0;36mXlibWindow.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_handlers[message] \u001b[38;5;241m=\u001b[39m func\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXlibWindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _can_detect_autorepeat\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_detect_autorepeat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/window/__init__.py:591\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m     config \u001b[38;5;241m=\u001b[39m screen\u001b[38;5;241m.\u001b[39mget_best_config(config)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context:\n\u001b[0;32m--> 591\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Set these in reverse order to above, to ensure we get user preference\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context \u001b[38;5;241m=\u001b[39m context\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/gl/xlib.py:206\u001b[0m, in \u001b[0;36mXlibCanvasConfig13.create_context\u001b[0;34m(self, share)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m XlibContextARB(\u001b[38;5;28mself\u001b[39m, share)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mXlibContext13\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/gl/xlib.py:314\u001b[0m, in \u001b[0;36mXlibContext13.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, share):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXlibContext13\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglx_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/MultiCar/lib/python3.10/site-packages/pyglet/gl/xlib.py:218\u001b[0m, in \u001b[0;36mBaseXlibContext.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglx_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_glx_context(share)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglx_context:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# TODO: Check Xlib error generated\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m gl\u001b[38;5;241m.\u001b[39mContextException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not create GL context\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_have_SGI_video_sync \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mglx_info\u001b[38;5;241m.\u001b[39mhave_extension(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGLX_SGI_video_sync\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_have_SGI_swap_control \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mglx_info\u001b[38;5;241m.\u001b[39mhave_extension(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGLX_SGI_swap_control\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mContextException\u001b[0m: Could not create GL context"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_multi_car_racing\n",
    "try:\n",
    "    env = gym.make(\"MultiCarRacing-v0\", num_agents=1, direction='CCW',\n",
    "                    use_random_direction=True, backwards_flag=True, h_ratio=0.25,\n",
    "                    use_ego_color=False)\n",
    "    SAC_exp = SAC(env, num_episodes=1, num_experiments=1, min_alpha=0.1, tau=0.1**(1/100), render_mode=None)\n",
    "finally:\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiCar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
